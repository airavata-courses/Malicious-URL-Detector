{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Refactored Intensive ML Model Script with Timestamped Outputs\n",
    "\n",
    "This script includes:\n",
    "    - Enhanced feature extraction from URLs.\n",
    "    - Data loading and preprocessing for a malicious URLs dataset.\n",
    "    - Multiple machine learning model training and evaluation (LDA, Logistic Regression, SVM, Random Forest).\n",
    "    - Energy and CO2 emissions tracking using CodeCarbon (if installed).\n",
    "    - Saving all generated plots and emissions logs into a timestamped folder inside the \"plots\" directory.\n",
    "\n",
    "Ensure the following packages are installed:\n",
    "    - CodeCarbon (for tracking energy/CO2 metrics): pip install codecarbon\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd  # type: ignore \n",
    "import numpy as np  # type: ignore \n",
    "import matplotlib.pyplot as plt  # type: ignore \n",
    "import seaborn as sns  # type: ignore \n",
    "from urllib.parse import urlparse  # type: ignore \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # type: ignore \n",
    "from sklearn.model_selection import train_test_split  # type: ignore \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # type: ignore \n",
    "from sklearn.decomposition import PCA  # type: ignore \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # type: ignore \n",
    "from sklearn.linear_model import LogisticRegression  # type: ignore \n",
    "from sklearn.svm import LinearSVC  # type: ignore \n",
    "from sklearn.ensemble import RandomForestClassifier  # type: ignore \n",
    "from sklearn.tree import plot_tree  # type: ignore \n",
    "from joblib import Parallel, delayed # type: ignore\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning messages matching a specific pattern\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*DataFrame concatenation with empty or all-NA entries.*\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker  # type: ignore \n",
    "except ImportError:\n",
    "    print(\"Warning: CodeCarbon is not installed. Install it via pip for energy/CO2 tracking.\")\n",
    "    EmissionsTracker = None\n",
    "\n",
    "\n",
    "# Utility Function to Save Plots\n",
    "def save_plot(fig, filename, folder):\n",
    "    \"\"\"\n",
    "    Save the figure to the specified folder with the given filename and close the figure.\n",
    "    \n",
    "    Parameters:\n",
    "        fig (matplotlib.figure.Figure): The plot figure.\n",
    "        filename (str): The file name to use (e.g., \"plot.png\").\n",
    "        folder (str): The directory where the plot will be saved.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    fig.savefig(filepath, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# --- URL Feature Extraction ---\n",
    "def extract_url_features(url):\n",
    "    \"\"\"\n",
    "    Extract enhanced features from a URL string.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to extract features from.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing various features derived from the URL.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # Basic length-based features\n",
    "    features['url_length'] = len(url)\n",
    "    features['domain_length'] = len(parsed.netloc)\n",
    "    features['path_length'] = len(parsed.path)\n",
    "    \n",
    "    # Count of numeric and special characters\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['num_special'] = len(re.findall(r'[^A-Za-z0-9]', url))\n",
    "    \n",
    "    # Domain structure features\n",
    "    features['num_dots'] = url.count('.')\n",
    "    features['num_hyphens'] = url.count('-')\n",
    "    features['num_underscores'] = url.count('_')\n",
    "    features['num_subdomains'] = len(parsed.netloc.split('.')) - 2  # Adjust as needed\n",
    "    \n",
    "    # URL path features\n",
    "    features['num_slashes'] = url.count('/')\n",
    "    features['num_params'] = len(parsed.params)\n",
    "    features['has_query'] = int(bool(parsed.query))\n",
    "    \n",
    "    # Protocol and IP-based features\n",
    "    features['uses_https'] = int(parsed.scheme == 'https')\n",
    "    features['has_ip_address'] = int(bool(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed.netloc)))\n",
    "    \n",
    "    # Suspicious keywords check\n",
    "    suspicious_words = ['login', 'secure', 'account', 'update', 'free', 'gift', 'verification']\n",
    "    features['suspicious_words'] = int(any(word in url.lower() for word in suspicious_words))\n",
    "    \n",
    "    # Calculate URL entropy\n",
    "    def calculate_entropy(s):\n",
    "        prob = [float(s.count(c)) / len(s) for c in dict.fromkeys(s)]\n",
    "        entropy = -sum(p * math.log(p, 2) for p in prob)\n",
    "        return entropy\n",
    "    features['url_entropy'] = calculate_entropy(url)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from a local directory, extract URL features in parallel using Joblib,\n",
    "    and preprocess the data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (df, X, y, X_train, X_test, y_train, y_test, \n",
    "                X_train_scaled, X_test_scaled, scaler, label_encoder)\n",
    "    \"\"\"\n",
    "    print(\"=== Data Loading and Preprocessing ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Define the local dataset directory and CSV file path\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    csv_path = os.path.join(script_dir, \"malicious_phish.csv\")\n",
    "    print(\"Using local dataset directory:\", script_dir)\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(\"Dataset loaded. First few rows:\\n\", df.head())\n",
    "    print(\"\\nDataset info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['type'].value_counts())\n",
    "    \n",
    "    # Extract enhanced features from URLs in parallel using Joblib\n",
    "    print(\"Extracting enhanced URL features using Joblib...\")\n",
    "    features_list = Parallel(n_jobs=-1, verbose=1)(delayed(extract_url_features)(url) for url in df['url'])\n",
    "    X = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Encode target labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['type'])\n",
    "    \n",
    "    # Split dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Standardize features for algorithms that require scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Preprocessing completed in {duration:.4f} seconds.\\n\")\n",
    "    \n",
    "    return df, X, y, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, le\n",
    "\n",
    "\n",
    "# --- Data Visualization ---\n",
    "def visualize_data(X, X_train, X_test, output_folder):\n",
    "    \"\"\"\n",
    "    Produce visualizations to understand feature correlations and distributions,\n",
    "    saving all plots to the specified output folder.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "        X_train (pd.DataFrame): Training feature set.\n",
    "        X_test (pd.DataFrame): Test feature set.\n",
    "        output_folder (str): Directory where plots will be saved.\n",
    "    \"\"\"\n",
    "    print(\"=== Data Visualization ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Plot the feature correlation heatmap\n",
    "    fig1 = plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig1, \"feature_correlation_matrix.png\", output_folder)\n",
    "    \n",
    "    # Plot boxplots for feature distributions\n",
    "    fig2 = plt.figure(figsize=(15, 10))\n",
    "    X.boxplot()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Feature Distributions')\n",
    "    save_plot(fig2, \"feature_distributions.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Data visualization completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "# --- Model Training and Evaluation Functions ---\n",
    "def run_lda_model(X_train, X_test, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Linear Discriminant Analysis (LDA) model.\n",
    "    \"\"\"\n",
    "    print(\"=== LDA Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"LDA Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - LDA')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"lda_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance (absolute average coefficient values)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(lda.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - LDA')\n",
    "    save_plot(fig_fi, \"lda_feature_importance.png\", output_folder)\n",
    "    \n",
    "    # LDA 2D Transformation Visualization (if applicable)\n",
    "    X_lda = lda.transform(X_train)\n",
    "    if X_lda.shape[1] >= 2:\n",
    "        fig_scatter = plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_train, cmap='viridis')\n",
    "        plt.title('LDA Transformation of Training Data')\n",
    "        plt.xlabel('First Discriminant')\n",
    "        plt.ylabel('Second Discriminant')\n",
    "        plt.colorbar(scatter, label='Classes')\n",
    "        save_plot(fig_scatter, \"lda_transformation.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"LDA model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_logistic_regression_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Logistic Regression model.\n",
    "    \"\"\"\n",
    "    print(\"=== Logistic Regression Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logreg = LogisticRegression(max_iter=1000)\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "    y_pred = logreg.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Logistic Regression Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - Logistic Regression')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"logreg_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance analysis using absolute coefficients\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(logreg.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - Logistic Regression')\n",
    "    save_plot(fig_fi, \"logreg_feature_importance.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Logistic Regression model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_svm_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a linear SVM model (LinearSVC).\n",
    "    \"\"\"\n",
    "    print(\"=== SVM Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False,\n",
    "                    C=1.0, tol=1e-4, max_iter=1000, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"LinearSVC Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - LinearSVC')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"svm_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance analysis using absolute coefficients\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(svm.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - LinearSVC')\n",
    "    plt.xlabel('Average Absolute Coefficient Value')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig_fi, \"svm_feature_importance.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"SVM model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_random_forest_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest Classifier model including additional visualizations.\n",
    "    \"\"\"\n",
    "    print(\"=== Random Forest Classifier Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Random Forest Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"rf_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - Random Forest')\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig_fi, \"rf_feature_importance.png\", output_folder)\n",
    "    \n",
    "    # Visualize a single decision tree from the Random Forest\n",
    "    fig_tree = plt.figure(figsize=(20, 10))\n",
    "    plot_tree(rf.estimators_[0], feature_names=feature_names, filled=True, max_depth=3, fontsize=10)\n",
    "    plt.title(\"Random Forest Tree Visualization (First Tree, max_depth=3)\")\n",
    "    save_plot(fig_tree, \"rf_tree_visualization.png\", output_folder)\n",
    "    \n",
    "    # Visualize decision boundaries using PCA reduction to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    \n",
    "    x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Map the grid back to the original feature space using PCA inverse transform\n",
    "    grid_original = pca.inverse_transform(grid_points)\n",
    "    Z = rf.predict(grid_original).reshape(xx.shape)\n",
    "    \n",
    "    fig_db = plt.figure(figsize=(12, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=20)\n",
    "    plt.title(\"Random Forest Decision Boundaries in PCA Space\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(scatter, label='Classes')\n",
    "    save_plot(fig_db, \"rf_decision_boundaries.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Random Forest model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the machine learning pipeline.\n",
    "    \n",
    "    Steps:\n",
    "        1. Define output directories with timestamp for unique run outputs.\n",
    "        2. Initialize the CodeCarbon emissions tracker (if installed) with the run-specific output directory.\n",
    "        3. Load, preprocess, and visualize the data.\n",
    "        4. Train and evaluate multiple machine learning models.\n",
    "        5. Report the overall execution time and CO2 emissions.\n",
    "    \"\"\"\n",
    "    overall_start_time = time.time()\n",
    "    \n",
    "    # Define the base directory (script directory) and a main plots folder\n",
    "    script_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "    plots_folder = os.path.join(script_dir, \"plots\")\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a timestamped subfolder for this specific run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_folder = os.path.join(plots_folder, timestamp)\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    print(\"Plots and logs will be saved to:\", run_folder)\n",
    "    \n",
    "    # Initialize CodeCarbon emissions tracker if available, using the run_folder for output\n",
    "    tracker = None\n",
    "    if EmissionsTracker is not None:\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=\"Cumulative ML Model\",\n",
    "            measure_power_secs=1,\n",
    "            log_level=\"error\",\n",
    "            output_dir=run_folder\n",
    "        )\n",
    "        tracker.start()\n",
    "    else:\n",
    "        print(\"Emissions tracking disabled due to missing CodeCarbon package.\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df, X, y, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, le = load_and_preprocess_data()\n",
    "    \n",
    "    # Data visualization\n",
    "    visualize_data(X, X_train, X_test, run_folder)\n",
    "    \n",
    "    # Model training and evaluation\n",
    "    run_lda_model(X_train, X_test, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_logistic_regression_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_svm_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_random_forest_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    \n",
    "    # Report overall metrics\n",
    "    total_time = time.time() - overall_start_time\n",
    "    if tracker is not None:\n",
    "        total_co2 = tracker.stop()\n",
    "        print(f\"Total CO2 Emissions for entire run: {total_co2:.4f} kg\")\n",
    "    else:\n",
    "        total_co2 = None\n",
    "    \n",
    "    print(f\"Total execution time: {total_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
