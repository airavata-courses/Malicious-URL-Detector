{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Loaded airavata_jupyter_magic (2.1.4.post4) \n",
      "(current runtime = local)\n",
      "\n",
      "  %authenticate                              -- Authenticate to access high-performance runtimes.\n",
      "  %request_runtime <rt> [args]               -- Request a runtime named <rt> with configuration <args>.\n",
      "                                                Call multiple times to request multiple runtimes.\n",
      "  %restart_runtime <rt>                      -- Restart runtime <rt> if it hangs. This will clear all variables.\n",
      "  %stop_runtime <rt>                         -- Stop runtime <rt> when no longer needed.\n",
      "  %wait_for_runtime <rt>                     -- Wait for runtime <rt> to be ready.\n",
      "  %switch_runtime <rt>                       -- Switch the active runtime to <rt>. All subsequent cells will run here.\n",
      "  %%run_on <rt>                              -- Force a cell to always execute on <rt>, regardless of the active runtime.\n",
      "  %stat_runtime <rt>                         -- Show the status of runtime <rt>.\n",
      "  %copy_data source=<r1:f1> target=<r2:f2>   -- Copy <f1> in <r1> to <f2> in <r2>.\n",
      "  %open_tunnels <tn> --forward=<ports>       -- Open a TCP tunnel on the runtime.\n",
      "  %close_tunnels <tn>                        -- Close a TCP tunnel opened on the runtime.\n",
      "  %run_subprocess <pn> --command=<cmd>\n",
      "                       --forward=<ports>     -- Start a subprocess on the runtime.\n",
      "  %kill_subprocess <pn>                      -- Kill a subprocess started on the runtime.\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86b83eb81ea9469c8e6bf266999504ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Authenticated.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Authenticated.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting runtime=hpc_cpu...\n",
      "[NeuroData25VC1:cloud, 60 Minutes, 1 Node(s), 4 CPU(s), 0 GPU(s), 4096 MB RAM, 1024 MB VRAM]\n",
      "* modules=[]\n",
      "* libraries=['python=3.10', 'pip', 'pandas', 'numpy', 'matplotlib', 'seaborn', 'scikit-learn', 'joblib']\n",
      "* pip=['codecarbon']\n",
      "* mounts=['airavata-courses-malicious-url-detector:/cybershuttle_data/airavata-courses-malicious-url-detector']\n",
      "Requested runtime=hpc_cpu\n",
      "Request successful: runtime=hpc_cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7f62e3ef2c4eaab9f7f5db1032f8d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local:/var/folders/_n/fcf6nx4j67gbbt4_8mjqxdc80000gn/T/connection_6z97omlt.json --> hpc_cpu:connection_6z97omlt.json... [200]\n",
      "started proc_name=hpc_cpu_kernel on rt=hpc_cpu. pid=2012\n",
      "forwarding ports=[24335, 24336, 24337, 24338, 24339]\n",
      "hpc_cpu:24335 -> access via 18.118.140.230:10000\n",
      "hpc_cpu:24336 -> access via 18.118.140.230:10001\n",
      "hpc_cpu:24337 -> access via 18.118.140.230:10002\n",
      "hpc_cpu:24338 -> access via 18.118.140.230:10003\n",
      "hpc_cpu:24339 -> access via 18.118.140.230:10004\n",
      "started ipykernel tunnels for hpc_cpu at 18.118.140.230\n",
      "started ipykernel client for hpc_cpu\n",
      "Remote Jupyter kernel launched and connected for runtime=hpc_cpu.\n",
      "Switched to runtime=hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU \"airavata-python-sdk[notebook]\"\n",
    "import airavata_jupyter_magic\n",
    "\n",
    "%authenticate\n",
    "%request_runtime hpc_cpu --file=cybershuttle.yml --walltime=60 --use=NeuroData25VC1:cloud\n",
    "%wait_for_runtime hpc_cpu --live\n",
    "%switch_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n",
      "test\n",
      "cell finished on hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n",
      "cell finished on hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n",
      "cell finished on hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "import math\n",
    "import datetime\n",
    "import pandas as pd  # type: ignore \n",
    "import numpy as np  # type: ignore \n",
    "import matplotlib.pyplot as plt  # type: ignore \n",
    "import seaborn as sns  # type: ignore \n",
    "from urllib.parse import urlparse  # type: ignore \n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler  # type: ignore \n",
    "from sklearn.model_selection import train_test_split  # type: ignore \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score  # type: ignore \n",
    "from sklearn.decomposition import PCA  # type: ignore \n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis  # type: ignore \n",
    "from sklearn.linear_model import LogisticRegression  # type: ignore \n",
    "from sklearn.svm import LinearSVC  # type: ignore \n",
    "from sklearn.ensemble import RandomForestClassifier  # type: ignore \n",
    "from sklearn.tree import plot_tree  # type: ignore \n",
    "from joblib import Parallel, delayed # type: ignore\n",
    "\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Suppress FutureWarning messages matching a specific pattern\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=\".*DataFrame concatenation with empty or all-NA entries.*\",\n",
    "    category=FutureWarning\n",
    ")\n",
    "\n",
    "try:\n",
    "    from codecarbon import EmissionsTracker  # type: ignore \n",
    "except ImportError:\n",
    "    print(\"Warning: CodeCarbon is not installed. Install it via pip for energy/CO2 tracking.\")\n",
    "    EmissionsTracker = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n",
      "cell finished on hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "# Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n",
      "cell finished on hpc_cpu.\n"
     ]
    }
   ],
   "source": [
    "# Utility Function to Save Plots\n",
    "def save_plot(fig, filename, folder):\n",
    "    \"\"\"\n",
    "    Save the figure to the specified folder with the given filename and close the figure.\n",
    "    \n",
    "    Parameters:\n",
    "        fig (matplotlib.figure.Figure): The plot figure.\n",
    "        filename (str): The file name to use (e.g., \"plot.png\").\n",
    "        folder (str): The directory where the plot will be saved.\n",
    "    \"\"\"\n",
    "    filepath = os.path.join(folder, filename)\n",
    "    fig.savefig(filepath, bbox_inches=\"tight\")\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# --- URL Feature Extraction ---\n",
    "def extract_url_features(url):\n",
    "    \"\"\"\n",
    "    Extract enhanced features from a URL string.\n",
    "    \n",
    "    Parameters:\n",
    "        url (str): The URL to extract features from.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing various features derived from the URL.\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    parsed = urlparse(url)\n",
    "    \n",
    "    # Basic length-based features\n",
    "    features['url_length'] = len(url)\n",
    "    features['domain_length'] = len(parsed.netloc)\n",
    "    features['path_length'] = len(parsed.path)\n",
    "    \n",
    "    # Count of numeric and special characters\n",
    "    features['num_digits'] = sum(c.isdigit() for c in url)\n",
    "    features['num_special'] = len(re.findall(r'[^A-Za-z0-9]', url))\n",
    "    \n",
    "    # Domain structure features\n",
    "    features['num_dots'] = url.count('.')\n",
    "    features['num_hyphens'] = url.count('-')\n",
    "    features['num_underscores'] = url.count('_')\n",
    "    features['num_subdomains'] = len(parsed.netloc.split('.')) - 2  # Adjust as needed\n",
    "    \n",
    "    # URL path features\n",
    "    features['num_slashes'] = url.count('/')\n",
    "    features['num_params'] = len(parsed.params)\n",
    "    features['has_query'] = int(bool(parsed.query))\n",
    "    \n",
    "    # Protocol and IP-based features\n",
    "    features['uses_https'] = int(parsed.scheme == 'https')\n",
    "    features['has_ip_address'] = int(bool(re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', parsed.netloc)))\n",
    "    \n",
    "    # Suspicious keywords check\n",
    "    suspicious_words = ['login', 'secure', 'account', 'update', 'free', 'gift', 'verification']\n",
    "    features['suspicious_words'] = int(any(word in url.lower() for word in suspicious_words))\n",
    "    \n",
    "    # Calculate URL entropy\n",
    "    def calculate_entropy(s):\n",
    "        prob = [float(s.count(c)) / len(s) for c in dict.fromkeys(s)]\n",
    "        entropy = -sum(p * math.log(p, 2) for p in prob)\n",
    "        return entropy\n",
    "    features['url_entropy'] = calculate_entropy(url)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "def load_and_preprocess_data():\n",
    "    \"\"\"\n",
    "    Load the dataset from the notebook’s working directory, extract URL features in parallel using Joblib,\n",
    "    and preprocess the data.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (df, X, y, X_train, X_test, y_train, y_test, \n",
    "                X_train_scaled, X_test_scaled, scaler, label_encoder)\n",
    "    \"\"\"\n",
    "    print(\"=== Data Loading and Preprocessing ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Use the notebook’s cwd as a base\n",
    "    script_dir = os.getcwd()\n",
    "    print(\"Files in working directory:\", os.listdir(script_dir))\n",
    "    \n",
    "    # Load the dataset\n",
    "    df = pd.read_csv(\"cybershuttle_data/airavata-courses-malicious-url-detector/malicious_phish.csv\")\n",
    "    print(\"Dataset loaded. First few rows:\\n\", df.head())\n",
    "    print(\"\\nDataset info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nClass distribution:\")\n",
    "    print(df['type'].value_counts())\n",
    "    \n",
    "    # Extract enhanced features from URLs in parallel using Joblib\n",
    "    print(\"Extracting enhanced URL features using Joblib...\")\n",
    "    features_list = Parallel(n_jobs=-1, verbose=1)(\n",
    "        delayed(extract_url_features)(url) for url in df['url']\n",
    "    )\n",
    "    X = pd.DataFrame(features_list)\n",
    "    \n",
    "    # Encode target labels\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(df['type'])\n",
    "    \n",
    "    # Split dataset into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Standardize features for algorithms that require scaling\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled  = scaler.transform(X_test)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Preprocessing completed in {duration:.4f} seconds.\\n\")\n",
    "    \n",
    "    return df, X, y, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, le\n",
    "\n",
    "\n",
    "# --- Data Visualization ---\n",
    "def visualize_data(X, X_train, X_test, output_folder):\n",
    "    \"\"\"\n",
    "    Produce visualizations to understand feature correlations and distributions,\n",
    "    saving all plots to the specified output folder.\n",
    "    \n",
    "    Args:\n",
    "        X (pd.DataFrame): Feature data.\n",
    "        X_train (pd.DataFrame): Training feature set.\n",
    "        X_test (pd.DataFrame): Test feature set.\n",
    "        output_folder (str): Directory where plots will be saved.\n",
    "    \"\"\"\n",
    "    print(\"=== Data Visualization ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Plot the feature correlation heatmap\n",
    "    fig1 = plt.figure(figsize=(12, 8))\n",
    "    sns.heatmap(X.corr(), annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Feature Correlation Matrix')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig1, \"feature_correlation_matrix.png\", output_folder)\n",
    "    \n",
    "    # Plot boxplots for feature distributions\n",
    "    fig2 = plt.figure(figsize=(15, 10))\n",
    "    X.boxplot()\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.title('Feature Distributions')\n",
    "    save_plot(fig2, \"feature_distributions.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Data visualization completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "# --- Model Training and Evaluation Functions ---\n",
    "def run_lda_model(X_train, X_test, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Linear Discriminant Analysis (LDA) model.\n",
    "    \"\"\"\n",
    "    print(\"=== LDA Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    lda = LinearDiscriminantAnalysis()\n",
    "    lda.fit(X_train, y_train)\n",
    "    y_pred = lda.predict(X_test)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"LDA Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - LDA')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"lda_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance (absolute average coefficient values)\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(lda.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - LDA')\n",
    "    save_plot(fig_fi, \"lda_feature_importance.png\", output_folder)\n",
    "    \n",
    "    # LDA 2D Transformation Visualization (if applicable)\n",
    "    X_lda = lda.transform(X_train)\n",
    "    if X_lda.shape[1] >= 2:\n",
    "        fig_scatter = plt.figure(figsize=(12, 8))\n",
    "        scatter = plt.scatter(X_lda[:, 0], X_lda[:, 1], c=y_train, cmap='viridis')\n",
    "        plt.title('LDA Transformation of Training Data')\n",
    "        plt.xlabel('First Discriminant')\n",
    "        plt.ylabel('Second Discriminant')\n",
    "        plt.colorbar(scatter, label='Classes')\n",
    "        save_plot(fig_scatter, \"lda_transformation.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"LDA model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_logistic_regression_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Logistic Regression model.\n",
    "    \"\"\"\n",
    "    print(\"=== Logistic Regression Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    logreg = LogisticRegression(max_iter=1000)\n",
    "    logreg.fit(X_train_scaled, y_train)\n",
    "    y_pred = logreg.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Logistic Regression Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - Logistic Regression')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"logreg_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance analysis using absolute coefficients\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(logreg.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - Logistic Regression')\n",
    "    save_plot(fig_fi, \"logreg_feature_importance.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Logistic Regression model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_svm_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a linear SVM model (LinearSVC).\n",
    "    \"\"\"\n",
    "    print(\"=== SVM Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    svm = LinearSVC(penalty='l2', loss='squared_hinge', dual=False,\n",
    "                    C=1.0, tol=1e-4, max_iter=1000, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"LinearSVC Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - LinearSVC')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"svm_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance analysis using absolute coefficients\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': np.abs(svm.coef_).mean(axis=0)\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - LinearSVC')\n",
    "    plt.xlabel('Average Absolute Coefficient Value')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig_fi, \"svm_feature_importance.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"SVM model training and evaluation completed in {duration:.4f} seconds.\\n\")\n",
    "\n",
    "\n",
    "def run_random_forest_model(X_train_scaled, X_test_scaled, y_train, y_test, le, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest Classifier model including additional visualizations.\n",
    "    \"\"\"\n",
    "    print(\"=== Random Forest Classifier Model ===\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train_scaled, y_train)\n",
    "    y_pred = rf.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Random Forest Accuracy: {acc * 100:.2f}%\")\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    fig_cm = plt.figure(figsize=(10, 8))\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "    plt.title('Confusion Matrix - Random Forest')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    save_plot(fig_cm, \"rf_confusion_matrix.png\", output_folder)\n",
    "    \n",
    "    # Feature Importance visualization\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': rf.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    fig_fi = plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=importance_df, x='Importance', y='Feature')\n",
    "    plt.title('Feature Importance - Random Forest')\n",
    "    plt.xlabel('Feature Importance Score')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    save_plot(fig_fi, \"rf_feature_importance.png\", output_folder)\n",
    "    \n",
    "    # Visualize a single decision tree from the Random Forest\n",
    "    fig_tree = plt.figure(figsize=(20, 10))\n",
    "    plot_tree(rf.estimators_[0], feature_names=feature_names, filled=True, max_depth=3, fontsize=10)\n",
    "    plt.title(\"Random Forest Tree Visualization (First Tree, max_depth=3)\")\n",
    "    save_plot(fig_tree, \"rf_tree_visualization.png\", output_folder)\n",
    "    \n",
    "    # Visualize decision boundaries using PCA reduction to 2D\n",
    "    pca = PCA(n_components=2)\n",
    "    X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "    \n",
    "    x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1\n",
    "    y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 300), np.linspace(y_min, y_max, 300))\n",
    "    grid_points = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    # Map the grid back to the original feature space using PCA inverse transform\n",
    "    grid_original = pca.inverse_transform(grid_points)\n",
    "    Z = rf.predict(grid_original).reshape(xx.shape)\n",
    "    \n",
    "    fig_db = plt.figure(figsize=(12, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap='coolwarm')\n",
    "    scatter = plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='viridis', edgecolor='k', s=20)\n",
    "    plt.title(\"Random Forest Decision Boundaries in PCA Space\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.colorbar(scatter, label='Classes')\n",
    "    save_plot(fig_db, \"rf_decision_boundaries.png\", output_folder)\n",
    "    \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Random Forest model training and evaluation completed in {duration:.4f} seconds.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "executing cell on hpc_cpu...\n",
      "waiting for cell to finish on hpc_cpu...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 04:38:08] Multiple instances of codecarbon are allowed to run at the same time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "Plots and logs will be saved to: /home/exouser/cybershuttle/workspace/PROCESS_a2f6bd9f-8458-459a-9516-964cb3f662aa/plots/20250512_043808\n",
      "=== Data Loading and Preprocessing ===\n",
      "Files in working directory: ['job_1938967087.slurm', 'A1344164005', 'AiravataAgent.stdout', 'AiravataAgent.stderr', 'application', 'cybershuttle_data', 'airavata-agent', 'kernel.py', 'micromamba', 'connection_6z97omlt.json', 'plots']\n",
      "Dataset loaded. First few rows:\n",
      "                                                  url        type\n",
      "0                                   br-icloud.com.br    phishing\n",
      "1                mp3raid.com/music/krizz_kaliko.html      benign\n",
      "2                    bopsecrets.org/rexroth/cr/1.htm      benign\n",
      "3  http://www.garage-pirenne.be/index.php?option=...  defacement\n",
      "4  http://adventure-nicaragua.net/index.php?optio...  defacement\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 651191 entries, 0 to 651190\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   url     651191 non-null  object\n",
      " 1   type    651191 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 9.9+ MB\n",
      "None\n",
      "\n",
      "Class distribution:\n",
      "type\n",
      "benign        428103\n",
      "defacement     96457\n",
      "phishing       94111\n",
      "malware        32520\n",
      "Name: count, dtype: int64\n",
      "Extracting enhanced URL features using Joblib...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=-1)]: Done 880 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 167920 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=-1)]: Done 645360 tasks      | elapsed:    5.0s\n",
      "[Parallel(n_jobs=-1)]: Done 651191 out of 651191 | elapsed:    5.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing completed in 8.0270 seconds.\n",
      "\n",
      "=== Data Visualization ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:38:21] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data visualization completed in 8.2247 seconds.\n",
      "\n",
      "=== LDA Model ===\n",
      "LDA Accuracy: 81.05%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.83      0.97      0.90     85778\n",
      "  defacement       0.78      0.91      0.84     19104\n",
      "     malware       0.66      0.57      0.61      6521\n",
      "    phishing       0.44      0.05      0.10     18836\n",
      "\n",
      "    accuracy                           0.81    130239\n",
      "   macro avg       0.68      0.63      0.61    130239\n",
      "weighted avg       0.76      0.81      0.76    130239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:38:30] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA model training and evaluation completed in 8.6024 seconds.\n",
      "\n",
      "=== Logistic Regression Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:38:38] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 83.24%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.85      0.97      0.91     85778\n",
      "  defacement       0.82      0.93      0.87     19104\n",
      "     malware       0.83      0.67      0.74      6521\n",
      "    phishing       0.52      0.15      0.23     18836\n",
      "\n",
      "    accuracy                           0.83    130239\n",
      "   macro avg       0.76      0.68      0.69    130239\n",
      "weighted avg       0.80      0.83      0.80    130239\n",
      "\n",
      "Logistic Regression model training and evaluation completed in 5.2958 seconds.\n",
      "\n",
      "=== SVM Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:38:46] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC Accuracy: 82.65%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.84      0.98      0.90     85778\n",
      "  defacement       0.80      0.95      0.87     19104\n",
      "     malware       0.85      0.54      0.66      6521\n",
      "    phishing       0.62      0.08      0.14     18836\n",
      "\n",
      "    accuracy                           0.83    130239\n",
      "   macro avg       0.78      0.64      0.64    130239\n",
      "weighted avg       0.80      0.83      0.78    130239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:38:54] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM model training and evaluation completed in 12.3126 seconds.\n",
      "\n",
      "=== Random Forest Classifier Model ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:39:02] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:10] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:18] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:26] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:34] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:42] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:50] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n",
      "[codecarbon ERROR @ 04:39:58] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 93.59%\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      benign       0.95      0.97      0.96     85778\n",
      "  defacement       0.97      0.99      0.98     19104\n",
      "     malware       0.98      0.94      0.96      6521\n",
      "    phishing       0.81      0.75      0.78     18836\n",
      "\n",
      "    accuracy                           0.94    130239\n",
      "   macro avg       0.93      0.91      0.92    130239\n",
      "weighted avg       0.93      0.94      0.93    130239\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:40:06] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest model training and evaluation completed in 75.0752 seconds.\n",
      "\n",
      "Total CO2 Emissions for entire run: 0.0010 kg\n",
      "Total execution time: 121.1434 seconds\n",
      "cell finished on hpc_cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon ERROR @ 04:40:10] Region:  not found for Country with ISO CODE : USA\n",
      "Traceback (most recent call last):\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 142, in get_private_infra_emissions\n",
      "    return self.get_region_emissions(energy, geo)\n",
      "  File \"/dev/shm/scratch/envs/d32ea28/lib/python3.10/site-packages/codecarbon/core/emissions.py\", line 168, in get_region_emissions\n",
      "    raise ValueError(\n",
      "ValueError: Region:  not found for Country with ISO CODE : USA\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to execute the machine learning pipeline.\n",
    "    \n",
    "    Steps:\n",
    "        1. Define output directories with timestamp for unique run outputs.\n",
    "        2. Initialize the CodeCarbon emissions tracker (if installed) with the run-specific output directory.\n",
    "        3. Load, preprocess, and visualize the data.\n",
    "        4. Train and evaluate multiple machine learning models.\n",
    "        5. Report the overall execution time and CO2 emissions.\n",
    "    \"\"\"\n",
    "    \n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    print(\"test\")\n",
    "    \n",
    "    # Define the base directory (script directory) and a main plots folder\n",
    "    script_dir = os.getcwd()\n",
    "    plots_folder = os.path.join(script_dir, \"plots\")\n",
    "    os.makedirs(plots_folder, exist_ok=True)\n",
    "    \n",
    "    # Create a timestamped subfolder for this specific run\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    run_folder = os.path.join(plots_folder, timestamp)\n",
    "    os.makedirs(run_folder, exist_ok=True)\n",
    "    print(\"Plots and logs will be saved to:\", run_folder)\n",
    "    \n",
    "    # Initialize CodeCarbon emissions tracker if available, using the run_folder for output\n",
    "    tracker = None\n",
    "    if EmissionsTracker is not None:\n",
    "        tracker = EmissionsTracker(\n",
    "            project_name=\"Cumulative ML Model\",\n",
    "            measure_power_secs=1,\n",
    "            log_level=\"error\",\n",
    "            output_dir=run_folder\n",
    "        )\n",
    "        tracker.start()\n",
    "    else:\n",
    "        print(\"Emissions tracking disabled due to missing CodeCarbon package.\")\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    df, X, y, X_train, X_test, y_train, y_test, X_train_scaled, X_test_scaled, scaler, le = load_and_preprocess_data()\n",
    "    \n",
    "    # Data visualization\n",
    "    visualize_data(X, X_train, X_test, run_folder)\n",
    "    \n",
    "    # Model training and evaluation\n",
    "    run_lda_model(X_train, X_test, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_logistic_regression_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_svm_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    run_random_forest_model(X_train_scaled, X_test_scaled, y_train, y_test, le, X.columns, run_folder)\n",
    "    \n",
    "    # Report overall metrics\n",
    "    total_time = time.time() - overall_start_time\n",
    "    if tracker is not None:\n",
    "        total_co2 = tracker.stop()\n",
    "        print(f\"Total CO2 Emissions for entire run: {total_co2:.4f} kg\")\n",
    "    else:\n",
    "        total_co2 = None\n",
    "    \n",
    "    print(f\"Total execution time: {total_time:.4f} seconds\")\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated runtime=hpc_cpu. state={'experimentId': 'CS_Agent_8f934580-8670-44e5-96c9-ebd4ae3b3a86', 'terminated': True}\n"
     ]
    }
   ],
   "source": [
    "%stop_runtime hpc_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
